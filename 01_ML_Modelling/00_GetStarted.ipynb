{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d5e1013-ddb8-4b99-b906-be734ec4d900",
   "metadata": {},
   "source": [
    "### Welcome to the CKW Energy Data Hackday Challenge\n",
    "This notebook is designed to guide you through the process of building machine learning models using pre-processed energy data. We will start by importing the data stored in Blob Storage and then apply a standard scaler to prepare it for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0f55b6f-5e2e-4783-b0c2-35218dbe973e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmath\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'joblib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math \n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f6380-bea7-4392-b6cd-732da9d9f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and show data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936a928-6c23-4b60-aaff-8cab8f77e161",
   "metadata": {},
   "source": [
    "The dataset consists of approximately 1'180'000 rows and 191 columns. The large number of columns is due to prior feature engineering, which has expanded the dataset. For a detailed view of the features that have been engineered, please refer to the 'FeatureEngineering' folder. Before we proceed with machine learning, it is essential to scale our data to ensure that all features contribute equally to the model's performance.\n",
    "\n",
    "#### Scaling the Data with StandardScaler\n",
    "To prepare our data for training, we will use the StandardScaler, which standardizes features by removing the mean and scaling to unit variance. This transformation is crucial for algorithms like XGBoost that are sensitive to the scale of input features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238e8c4-d60a-4b34-a5eb-553fc6af6e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data with a standard scaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  \n",
    "X_val_scaled = scaler.transform(X_val)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26765bab-e799-46ab-8402-b253e41e5cbb",
   "metadata": {},
   "source": [
    "#### Building an XGBoost Regressor Model\n",
    "In this section, we will construct our machine learning model using the XGBoost library. XGBoost is a powerful and efficient implementation of gradient boosting, widely used for regression and classification tasks. After initializing the XGBoost Regressor with specified parameters, we will train the model using the scaled training data. Once trained, we will make predictions on the validation set to evaluate the model's performance.To assess how well our model is performing, we will calculate several performance metrics, including Mean Squared Error (MSE), R-squared (R2), and Mean Absolute Error (MAE). These metrics will provide insights into the accuracy of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c21ad9-373a-44ae-ab63-8daf3b7a5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize XGBoost Regressor model\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    objective='reg:squarederror'\n",
    ")\n",
    "\n",
    "# train model on scaled train data\n",
    "model.fit(X_train_scaled, y_Train)\n",
    "\n",
    "# predict y with validation data\n",
    "y_pred = model.predict(X_val_scaled)\n",
    "\n",
    "# measure performance values\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "print(f\"Validation MSE: {mse:.4f}\")\n",
    "print(f\"Validation R2: {r2:.4f}\")\n",
    "print(f\"Validation MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ce829-f8f8-41f3-a79b-3e30525829be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save Model\n",
    "# model.save_model(\"02_TrainedModels/xgb_regressor_model_with_feed_in.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bee04a-6234-42f1-a80b-6ea38089a5c1",
   "metadata": {},
   "source": [
    "#### Understanding Feature Importances\n",
    "In this step, we will analyze which features have the most significant influence on the predicted values. Understanding feature importance helps in interpreting the model and identifying the most impactful variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3afc51-18f1-4b10-b306-9215fda99af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "feature_names = X_train.columns if hasattr(X_train, 'columns') else [f'feature_{i}' for i in range(X_train.shape[1])]\n",
    "\n",
    "# Sortieren\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "top_n = 15  # Anzahl der Top-Features\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(range(top_n), importances[sorted_idx][:top_n], color='skyblue')\n",
    "plt.xticks(range(top_n), [feature_names[i] for i in sorted_idx][:top_n], rotation=45, ha='right')\n",
    "plt.title('Top Feature Importances')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Werte über Balken anzeigen\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height, f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e174ea8e-6dbc-4b66-aa04-ce81995f6d7c",
   "metadata": {},
   "source": [
    "As you can see, the feed in feature has the biggest impact on the final predicted value. also the global radiation and the panel peak power have good influences on the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c0c090-2337-423f-8696-88ec9d7282f2",
   "metadata": {},
   "source": [
    "#### Model Without Feed-in Feature\n",
    "As a rigorous test, we will attempt to forecast values without using the feed-in energy feature. This approach will help us understand the model's robustness and the importance of various features in making accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e4ce1-7515-43d2-ad30-31aa70ca40f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove feed in features \n",
    "ueberschuss_cols = [col for col in X_train.columns if 'Überschuss' in col]\n",
    "cols_to_remove = ueberschuss_cols + ['feed_in:kWh']\n",
    "cols_to_keep = [col for col in X_train.columns if col not in cols_to_remove]\n",
    "\n",
    "X_train_scaled_without_feed_in = X_train_scaled[:, [X_train.columns.get_loc(col) for col in cols_to_keep]]\n",
    "X_val_scaled_without_feed_in = X_val_scaled[:, [X_val.columns.get_loc(col) for col in cols_to_keep]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc8f36-117b-49bc-919e-279e9ab20343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize XGBoost Regressor model\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    objective='reg:squarederror'\n",
    ")\n",
    "\n",
    "# train model on scaled train data\n",
    "model.fit(X_train_scaled_without_feed_in, y_Train)\n",
    "\n",
    "# predict y with validation data\n",
    "y_pred_wo_feed_in = model.predict(X_val_scaled_without_feed_in)\n",
    "\n",
    "# measure performance values\n",
    "mse = mean_squared_error(y_val, y_pred_wo_feed_in)\n",
    "r2 = r2_score(y_val, y_pred_wo_feed_in)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "print(f\"Validation MSE: {mse:.4f}\")\n",
    "print(f\"Validation R2: {r2:.4f}\")\n",
    "print(f\"Validation MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48533699-60f0-4453-bb5a-9f3b73c67b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save model\n",
    "# model.save_model(\"02_TrainedModels/xgb_regressor_model_without_feed_in.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17fbbb-1967-4186-a139-74de535cb9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "feature_names = cols_to_keep\n",
    "\n",
    "# Sortieren\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "top_n = 15  # Anzahl der Top-Features\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(range(top_n), importances[sorted_idx][:top_n], color='skyblue')\n",
    "plt.xticks(range(top_n), [feature_names[i] for i in sorted_idx][:top_n], rotation=45, ha='right')\n",
    "plt.title('Top Feature Importances')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Werte über Balken anzeigen\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height, f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b1099c-ca51-44ae-bf46-522608c6b61b",
   "metadata": {},
   "source": [
    "As you can see the model performs worse, because very important features are missing now. The key discipline is to improve this model. To visualize the model's performance, we will create scatter plots comparing the true values against the predicted values for both the model with and without the feed-in feature. This visualization will help us understand the model's accuracy and areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef42f17b-8fda-4269-ab72-1ee429bcd963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a scatterplot to visualize the true vs the predicted values of both models\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val, y_pred_wo_feed_in, alpha=0.1, color='blue', label='Without feed-in values')\n",
    "plt.scatter(y_val, y_pred, alpha=0.1, color='green', label='With feed-in values')\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# without feed in feature\n",
    "axes[0].scatter(y_val, y_pred_wo_feed_in, alpha=0.1, color='blue')\n",
    "axes[0].plot([0, 12], [0, 12], linestyle='--', color='red', label='perfect model')\n",
    "axes[0].set_title('Without feed-in values')\n",
    "axes[0].set_xlabel('True value')\n",
    "axes[0].set_ylabel('Predicted value')\n",
    "axes[0].legend()\n",
    "axes[0].grid()\n",
    "\n",
    "# model with feed in feature\n",
    "axes[1].scatter(y_val, y_pred, alpha=0.1, color='green')\n",
    "axes[1].plot([0, 12], [0, 12], linestyle='--', color='red', label='perfect model')\n",
    "axes[1].set_title('Without feed-in values')\n",
    "axes[1].set_xlabel('True value')\n",
    "axes[1].set_ylabel('Predicted value')\n",
    "axes[1].legend()\n",
    "axes[1].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edad6b4-d529-4a65-a44c-cf44ea96a463",
   "metadata": {},
   "source": [
    "#### Visualize Model on Test Data\n",
    "Now it is the Goal to visualize the predicted data on test data. In this case we can see if the performance could really work on totally different data it was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6febcd38-79be-4af3-a709-1f9a7de3188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read cleaned test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e8997b-6bde-4d05-9b68-e373fef13a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true data could also have some different features that the model was not trained on, so we need to match the features\n",
    "train_columns = train_columns_df['column'].str.strip()\n",
    "\n",
    "# convert train_columns to a pandas Index for set operations\n",
    "train_columns = pd.Index(train_columns)\n",
    "\n",
    "# identify missing columns in the test data\n",
    "missing_columns = train_columns.difference(df_edh25_test.columns)\n",
    "\n",
    "# identify new columns in the test data\n",
    "new_columns = df_edh25_test.columns.difference(train_columns)\n",
    "\n",
    "for col in missing_columns:\n",
    "    df_edh25_test[col] = 0  # or use another default value\n",
    "\n",
    "df_edh25_test_model = df_edh25_test.drop(columns=new_columns, errors='ignore')\n",
    "\n",
    "df_edh25_test_model = df_edh25_test_model.reindex(columns=train_columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c113f9-04ce-47d7-b0bf-973192fae59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(df_edh25_test_model)  # Wende den Scaler auf neue Daten an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e006c6c6-cd6d-45dd-9e60-c92dfa1ae6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data in DMatrix and predict values\n",
    "dtest = xgb.DMatrix(X_test_scaled)  \n",
    "predictions = model.predict(dtest)  \n",
    "\n",
    "# calculate metrics\n",
    "y_true = df_edh25_test['feed_in:kWh'].values\n",
    "r2 = r2_score(y_true, predictions)\n",
    "mae = mean_absolute_error(y_true, predictions)\n",
    "mse = mean_squared_error(y_true, predictions)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'r2_score': [r2],\n",
    "    'mae': [mae],\n",
    "    'mse': [mse]\n",
    "})\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b18201-a800-414f-878f-53cc4637c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame({\n",
    "    'id': df_edh25_test['id'], \n",
    "    'predicted_value': predictions\n",
    "})\n",
    "\n",
    "# Merge the predictions with the main DataFrame based on index and remove the second 'id' column if duplicated\n",
    "df_main_with_predictions = pd.concat([df_edh25_test.reset_index(drop=True), predictions_df.reset_index(drop=True)], axis=1)\n",
    "if df_main_with_predictions.columns.duplicated().any():\n",
    "    df_main_with_predictions = df_main_with_predictions.loc[:, ~df_main_with_predictions.columns.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345879c-a172-48e2-aacb-a8372a99891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_with_predictions['datum'] = pd.to_datetime(df_main_with_predictions['datum'])\n",
    "\n",
    "# Filter for the week of interest\n",
    "start_date = '2025-07-07'\n",
    "end_date = '2025-07-13'\n",
    "df_week = df_main_with_predictions[(df_main_with_predictions['datum'] >= start_date) & \n",
    "                                    (df_main_with_predictions['datum'] <= end_date)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8de7526-8a1a-4c9f-8241-0a476901f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_val, group in df_week_short.groupby('id'):\n",
    "    group = group.sort_values('datum')\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(group['datum'], group['feed_in:kWh'], label='Feed-In', marker='o')\n",
    "    ax.plot(group['datum'], group['predicted_value'], linestyle='--', label='Predicted Production', marker='x')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('kWh')\n",
    "    ax.set_title(f'Actual vs Predicted feed_in:kWh for ID {id_val} (2025-07-14 to 2025-07-20)')\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    display(fig)\n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
