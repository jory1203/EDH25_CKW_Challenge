{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d5e1013-ddb8-4b99-b906-be734ec4d900",
   "metadata": {},
   "source": [
    "### Welcome to the CKW Energy Data Hackday Challenge\n",
    "This notebook is designed to guide you through the process of building machine learning models using pre-processed energy data. We will start by importing the data stored in Blob Storage and then apply a standard scaler to prepare it for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d555a061-ed29-437f-9a0d-58fb26aadda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from -r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 1)) (2.3.1)\n",
      "Requirement already satisfied: numpy in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from -r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 2)) (2.3.2)\n",
      "Requirement already satisfied: joblib in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from -r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 3)) (1.5.1)\n",
      "Requirement already satisfied: matplotlib in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from -r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 4)) (3.10.5)\n",
      "Requirement already satisfied: scikit-learn in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from -r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 5)) (1.7.1)\n",
      "Requirement already satisfied: xgboost in /home/renku/work/.venv/lib/python3.13/site-packages (from -r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 6)) (3.0.4)\n",
      "Requirement already satisfied: holidays in /home/renku/work/.venv/lib/python3.13/site-packages (from -r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 7)) (0.80)\n",
      "Requirement already satisfied: pyarrow in /home/renku/work/.venv/lib/python3.13/site-packages (from -r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 8)) (21.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from pandas->-r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from pandas->-r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from pandas->-r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from matplotlib->-r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from matplotlib->-r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from matplotlib->-r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 4)) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from matplotlib->-r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 4)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from matplotlib->-r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from matplotlib->-r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 4)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from matplotlib->-r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 4)) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from scikit-learn->-r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 5)) (1.16.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from scikit-learn->-r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/renku/work/.venv/lib/python3.13/site-packages (from xgboost->-r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 6)) (2.27.7)\n",
      "Requirement already satisfied: six>=1.5 in /layers/paketo-buildpacks_poetry-install/poetry-venv/datascience-python-renku-dependencies-xS3fZVNL-py3.13/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->-r /home/renku/work/EDH25_CKW_Challenge/requirements.txt (line 1)) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r /home/renku/work/EDH25_CKW_Challenge/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0f55b6f-5e2e-4783-b0c2-35218dbe973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math \n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c85e9f4-b7a5-4e9a-8d08-95cd18aec4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from blob storage\n",
    "paths = {\n",
    "    \"X_train\": \"/home/renku/work/ckw-pv-data/source-data/EDH25/x_train.parquet\",\n",
    "    \"X_val\": \"/home/renku/work/ckw-pv-data/source-data/EDH25/x_val.parquet\",\n",
    "    \"y_train\": \"/home/renku/work/ckw-pv-data/source-data/EDH25/x_train.parquet\",\n",
    "    \"y_val\": \"/home/renku/work/ckw-pv-data/source-data/EDH25/x_val.parquet\"\n",
    "}\n",
    "\n",
    "X_train = pd.read_parquet(paths[\"X_train\"])\n",
    "X_val = pd.read_parquet(paths[\"X_val\"])\n",
    "y_train = pd.read_parquet(paths[\"y_train\"])\n",
    "y_val = pd.read_parquet(paths[\"y_val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936a928-6c23-4b60-aaff-8cab8f77e161",
   "metadata": {},
   "source": [
    "The dataset consists of approximately 1'180'000 rows and 209 columns. The large number of columns is due to prior feature engineering, which has expanded the dataset. For a detailed view of the features that have been engineered, please refer to the 'FeatureEngineering' folder. Before we proceed with machine learning, it is essential to scale our data to ensure that all features contribute equally to the model's performance.\n",
    "\n",
    "#### Scaling the Data with StandardScaler\n",
    "To prepare our data for training, we will use the StandardScaler, which standardizes features by removing the mean and scaling to unit variance. This transformation is crucial for algorithms like XGBoost that are sensitive to the scale of input features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b84fff9f-159f-47c7-9e27-d40ccf269c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "batch_size = 10000  \n",
    "for start in range(0, len(X_train), batch_size):\n",
    "    end = start + batch_size\n",
    "    scaler.partial_fit(X_train[start:end])\n",
    "    \n",
    "X_train_scaled = []\n",
    "for start in range(0, len(X_train), batch_size):\n",
    "    end = start + batch_size\n",
    "    X_train_scaled.append(scaler.transform(X_train[start:end]))\n",
    "\n",
    "X_train_scaled = np.vstack(X_train_scaled)\n",
    "X_val_scaled = scaler.transform(X_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26765bab-e799-46ab-8402-b253e41e5cbb",
   "metadata": {},
   "source": [
    "#### Building an XGBoost Regressor Model\n",
    "In this section, we will construct our machine learning model using the XGBoost library. XGBoost is a powerful and efficient implementation of gradient boosting, widely used for regression and classification tasks. After initializing the XGBoost Regressor with specified parameters, we will train the model using the scaled training data. Once trained, we will make predictions on the validation set to evaluate the model's performance.To assess how well our model is performing, we will calculate several performance metrics, including Mean Squared Error (MSE), R-squared (R2), and Mean Absolute Error (MAE). These metrics will provide insights into the accuracy of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4c21ad9-373a-44ae-ab63-8daf3b7a5aef",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      2\u001b[39m model = xgb.XGBRegressor(\n\u001b[32m      3\u001b[39m     n_estimators=\u001b[32m100\u001b[39m,\n\u001b[32m      4\u001b[39m     max_depth=\u001b[32m6\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m     objective=\u001b[33m'\u001b[39m\u001b[33mreg:squarederror\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# train model on scaled train data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# predict y with validation data\u001b[39;00m\n\u001b[32m     14\u001b[39m y_pred = model.predict(X_val_scaled)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.13/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.13/site-packages/xgboost/sklearn.py:1247\u001b[39m, in \u001b[36mXGBModel.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[39m\n\u001b[32m   1244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1245\u001b[39m     obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1247\u001b[39m \u001b[38;5;28mself\u001b[39m._Booster = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1258\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28mself\u001b[39m._set_evaluation_result(evals_result)\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.13/site-packages/xgboost/core.py:729\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    728\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m729\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.13/site-packages/xgboost/training.py:183\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[43mbst\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/.venv/lib/python3.13/site-packages/xgboost/core.py:2247\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2243\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2245\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2246\u001b[39m     _check_call(\n\u001b[32m-> \u001b[39m\u001b[32m2247\u001b[39m         \u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2250\u001b[39m     )\n\u001b[32m   2251\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2252\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# initialize XGBoost Regressor model\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    objective='reg:squarederror'\n",
    ")\n",
    "\n",
    "# train model on scaled train data\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# predict y with validation data\n",
    "y_pred = model.predict(X_val_scaled)\n",
    "\n",
    "# measure performance values\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "print(f\"Validation MSE: {mse:.4f}\")\n",
    "print(f\"Validation R2: {r2:.4f}\")\n",
    "print(f\"Validation MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ce829-f8f8-41f3-a79b-3e30525829be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save Model\n",
    "# model.save_model(\"02_TrainedModels/xgb_regressor_model_with_feed_in.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bee04a-6234-42f1-a80b-6ea38089a5c1",
   "metadata": {},
   "source": [
    "#### Understanding Feature Importances\n",
    "In this step, we will analyze which features have the most significant influence on the predicted values. Understanding feature importance helps in interpreting the model and identifying the most impactful variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3afc51-18f1-4b10-b306-9215fda99af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "feature_names = X_train.columns if hasattr(X_train, 'columns') else [f'feature_{i}' for i in range(X_train.shape[1])]\n",
    "\n",
    "# Sortieren\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "top_n = 15  # Anzahl der Top-Features\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(range(top_n), importances[sorted_idx][:top_n], color='skyblue')\n",
    "plt.xticks(range(top_n), [feature_names[i] for i in sorted_idx][:top_n], rotation=45, ha='right')\n",
    "plt.title('Top Feature Importances')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Werte über Balken anzeigen\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height, f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e174ea8e-6dbc-4b66-aa04-ce81995f6d7c",
   "metadata": {},
   "source": [
    "As you can see, the feed in feature has the biggest impact on the final predicted value. also the global radiation and the panel peak power have good influences on the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c0c090-2337-423f-8696-88ec9d7282f2",
   "metadata": {},
   "source": [
    "#### Model Without Feed-in Feature\n",
    "As a rigorous test, we will attempt to forecast values without using the feed-in energy feature. This approach will help us understand the model's robustness and the importance of various features in making accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e4ce1-7515-43d2-ad30-31aa70ca40f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove feed in features \n",
    "ueberschuss_cols = [col for col in X_train.columns if 'Überschuss' in col]\n",
    "cols_to_remove = ueberschuss_cols + ['feed_in:kWh']\n",
    "cols_to_keep = [col for col in X_train.columns if col not in cols_to_remove]\n",
    "\n",
    "X_train_scaled_without_feed_in = X_train_scaled[:, [X_train.columns.get_loc(col) for col in cols_to_keep]]\n",
    "X_val_scaled_without_feed_in = X_val_scaled[:, [X_val.columns.get_loc(col) for col in cols_to_keep]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc8f36-117b-49bc-919e-279e9ab20343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize XGBoost Regressor model\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    objective='reg:squarederror'\n",
    ")\n",
    "\n",
    "# train model on scaled train data\n",
    "model.fit(X_train_scaled_without_feed_in, y_Train)\n",
    "\n",
    "# predict y with validation data\n",
    "y_pred_wo_feed_in = model.predict(X_val_scaled_without_feed_in)\n",
    "\n",
    "# measure performance values\n",
    "mse = mean_squared_error(y_val, y_pred_wo_feed_in)\n",
    "r2 = r2_score(y_val, y_pred_wo_feed_in)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "print(f\"Validation MSE: {mse:.4f}\")\n",
    "print(f\"Validation R2: {r2:.4f}\")\n",
    "print(f\"Validation MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48533699-60f0-4453-bb5a-9f3b73c67b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save model\n",
    "# model.save_model(\"02_TrainedModels/xgb_regressor_model_without_feed_in.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17fbbb-1967-4186-a139-74de535cb9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "feature_names = cols_to_keep\n",
    "\n",
    "# Sortieren\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "top_n = 15  # Anzahl der Top-Features\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(range(top_n), importances[sorted_idx][:top_n], color='skyblue')\n",
    "plt.xticks(range(top_n), [feature_names[i] for i in sorted_idx][:top_n], rotation=45, ha='right')\n",
    "plt.title('Top Feature Importances')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Werte über Balken anzeigen\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height, f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b1099c-ca51-44ae-bf46-522608c6b61b",
   "metadata": {},
   "source": [
    "As you can see the model performs worse, because very important features are missing now. The key discipline is to improve this model. To visualize the model's performance, we will create scatter plots comparing the true values against the predicted values for both the model with and without the feed-in feature. This visualization will help us understand the model's accuracy and areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef42f17b-8fda-4269-ab72-1ee429bcd963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a scatterplot to visualize the true vs the predicted values of both models\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val, y_pred_wo_feed_in, alpha=0.1, color='blue', label='Without feed-in values')\n",
    "plt.scatter(y_val, y_pred, alpha=0.1, color='green', label='With feed-in values')\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# without feed in feature\n",
    "axes[0].scatter(y_val, y_pred_wo_feed_in, alpha=0.1, color='blue')\n",
    "axes[0].plot([0, 12], [0, 12], linestyle='--', color='red', label='perfect model')\n",
    "axes[0].set_title('Without feed-in values')\n",
    "axes[0].set_xlabel('True value')\n",
    "axes[0].set_ylabel('Predicted value')\n",
    "axes[0].legend()\n",
    "axes[0].grid()\n",
    "\n",
    "# model with feed in feature\n",
    "axes[1].scatter(y_val, y_pred, alpha=0.1, color='green')\n",
    "axes[1].plot([0, 12], [0, 12], linestyle='--', color='red', label='perfect model')\n",
    "axes[1].set_title('Without feed-in values')\n",
    "axes[1].set_xlabel('True value')\n",
    "axes[1].set_ylabel('Predicted value')\n",
    "axes[1].legend()\n",
    "axes[1].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edad6b4-d529-4a65-a44c-cf44ea96a463",
   "metadata": {},
   "source": [
    "#### Visualize Model on Test Data\n",
    "Now it is the Goal to visualize the predicted data on test data. In this case we can see if the performance could really work on totally different data it was trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6febcd38-79be-4af3-a709-1f9a7de3188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read cleaned test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e8997b-6bde-4d05-9b68-e373fef13a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# true data could also have some different features that the model was not trained on, so we need to match the features\n",
    "train_columns = train_columns_df['column'].str.strip()\n",
    "\n",
    "# convert train_columns to a pandas Index for set operations\n",
    "train_columns = pd.Index(train_columns)\n",
    "\n",
    "# identify missing columns in the test data\n",
    "missing_columns = train_columns.difference(df_edh25_test.columns)\n",
    "\n",
    "# identify new columns in the test data\n",
    "new_columns = df_edh25_test.columns.difference(train_columns)\n",
    "\n",
    "for col in missing_columns:\n",
    "    df_edh25_test[col] = 0  # or use another default value\n",
    "\n",
    "df_edh25_test_model = df_edh25_test.drop(columns=new_columns, errors='ignore')\n",
    "\n",
    "df_edh25_test_model = df_edh25_test_model.reindex(columns=train_columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c113f9-04ce-47d7-b0bf-973192fae59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(df_edh25_test_model)  # Wende den Scaler auf neue Daten an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e006c6c6-cd6d-45dd-9e60-c92dfa1ae6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data in DMatrix and predict values\n",
    "dtest = xgb.DMatrix(X_test_scaled)  \n",
    "predictions = model.predict(dtest)  \n",
    "\n",
    "# calculate metrics\n",
    "y_true = df_edh25_test['feed_in:kWh'].values\n",
    "r2 = r2_score(y_true, predictions)\n",
    "mae = mean_absolute_error(y_true, predictions)\n",
    "mse = mean_squared_error(y_true, predictions)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'r2_score': [r2],\n",
    "    'mae': [mae],\n",
    "    'mse': [mse]\n",
    "})\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b18201-a800-414f-878f-53cc4637c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame({\n",
    "    'id': df_edh25_test['id'], \n",
    "    'predicted_value': predictions\n",
    "})\n",
    "\n",
    "# Merge the predictions with the main DataFrame based on index and remove the second 'id' column if duplicated\n",
    "df_main_with_predictions = pd.concat([df_edh25_test.reset_index(drop=True), predictions_df.reset_index(drop=True)], axis=1)\n",
    "if df_main_with_predictions.columns.duplicated().any():\n",
    "    df_main_with_predictions = df_main_with_predictions.loc[:, ~df_main_with_predictions.columns.duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345879c-a172-48e2-aacb-a8372a99891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main_with_predictions['datum'] = pd.to_datetime(df_main_with_predictions['datum'])\n",
    "\n",
    "# Filter for the week of interest\n",
    "start_date = '2025-07-07'\n",
    "end_date = '2025-07-13'\n",
    "df_week = df_main_with_predictions[(df_main_with_predictions['datum'] >= start_date) & \n",
    "                                    (df_main_with_predictions['datum'] <= end_date)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8de7526-8a1a-4c9f-8241-0a476901f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_val, group in df_week_short.groupby('id'):\n",
    "    group = group.sort_values('datum')\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(group['datum'], group['feed_in:kWh'], label='Feed-In', marker='o')\n",
    "    ax.plot(group['datum'], group['predicted_value'], linestyle='--', label='Predicted Production', marker='x')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('kWh')\n",
    "    ax.set_title(f'Actual vs Predicted feed_in:kWh for ID {id_val} (2025-07-14 to 2025-07-20)')\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    display(fig)\n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
