{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d5e1013-ddb8-4b99-b906-be734ec4d900",
   "metadata": {},
   "source": [
    "### Welcome to the CKW Energy Data Hackday Challenge\n",
    "This notebook is designed to guide you through the process of building machine learning models using pre-processed energy data. We will start by importing the data stored in Blob Storage and then apply a standard scaler to prepare it for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0f55b6f-5e2e-4783-b0c2-35218dbe973e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmath\u001b[39;00m \n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'joblib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math \n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f6380-bea7-4392-b6cd-732da9d9f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and show data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936a928-6c23-4b60-aaff-8cab8f77e161",
   "metadata": {},
   "source": [
    "The dataset consists of approximately 1'180'000 rows and 191 columns. The large number of columns is due to prior feature engineering, which has expanded the dataset. For a detailed view of the features that have been engineered, please refer to the 'FeatureEngineering' folder. Before we proceed with machine learning, it is essential to scale our data to ensure that all features contribute equally to the model's performance.\n",
    "\n",
    "#### Scaling the Data with StandardScaler\n",
    "To prepare our data for training, we will use the StandardScaler, which standardizes features by removing the mean and scaling to unit variance. This transformation is crucial for algorithms like XGBoost that are sensitive to the scale of input features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3238e8c4-d60a-4b34-a5eb-553fc6af6e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data with a standard scaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  \n",
    "X_val_scaled = scaler.transform(X_val)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26765bab-e799-46ab-8402-b253e41e5cbb",
   "metadata": {},
   "source": [
    "#### Building an XGBoost Regressor Model\n",
    "In this section, we will construct our machine learning model using the XGBoost library. XGBoost is a powerful and efficient implementation of gradient boosting, widely used for regression and classification tasks. After initializing the XGBoost Regressor with specified parameters, we will train the model using the scaled training data. Once trained, we will make predictions on the validation set to evaluate the model's performance.To assess how well our model is performing, we will calculate several performance metrics, including Mean Squared Error (MSE), R-squared (R2), and Mean Absolute Error (MAE). These metrics will provide insights into the accuracy of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c21ad9-373a-44ae-ab63-8daf3b7a5aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize XGBoost Regressor model\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    objective='reg:squarederror'\n",
    ")\n",
    "\n",
    "# train model on scaled train data\n",
    "model.fit(X_train_scaled, y_Train)\n",
    "\n",
    "# predict y with validation data\n",
    "y_pred = model.predict(X_val_scaled)\n",
    "\n",
    "# measure performance values\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "print(f\"Validation MSE: {mse:.4f}\")\n",
    "print(f\"Validation R2: {r2:.4f}\")\n",
    "print(f\"Validation MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ce829-f8f8-41f3-a79b-3e30525829be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save Model\n",
    "# model.save_model(\"01_Models/xgb_regressor_model_with_feed_in.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bee04a-6234-42f1-a80b-6ea38089a5c1",
   "metadata": {},
   "source": [
    "#### Understanding Feature Importances\n",
    "In this step, we will analyze which features have the most significant influence on the predicted values. Understanding feature importance helps in interpreting the model and identifying the most impactful variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3afc51-18f1-4b10-b306-9215fda99af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "feature_names = X_train.columns if hasattr(X_train, 'columns') else [f'feature_{i}' for i in range(X_train.shape[1])]\n",
    "\n",
    "# Sortieren\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "top_n = 15  # Anzahl der Top-Features\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(range(top_n), importances[sorted_idx][:top_n], color='skyblue')\n",
    "plt.xticks(range(top_n), [feature_names[i] for i in sorted_idx][:top_n], rotation=45, ha='right')\n",
    "plt.title('Top Feature Importances')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Werte über Balken anzeigen\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height, f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e174ea8e-6dbc-4b66-aa04-ce81995f6d7c",
   "metadata": {},
   "source": [
    "As you can see, the feed in feature has the biggest impact on the final predicted value. also the global radiation and the panel peak power have good influences on the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c0c090-2337-423f-8696-88ec9d7282f2",
   "metadata": {},
   "source": [
    "#### Model Without Feed-in Feature\n",
    "As a rigorous test, we will attempt to forecast values without using the feed-in energy feature. This approach will help us understand the model's robustness and the importance of various features in making accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e4ce1-7515-43d2-ad30-31aa70ca40f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove feed in features \n",
    "ueberschuss_cols = [col for col in X_train.columns if 'Überschuss' in col]\n",
    "cols_to_remove = ueberschuss_cols + ['feed_in:kWh']\n",
    "cols_to_keep = [col for col in X_train.columns if col not in cols_to_remove]\n",
    "\n",
    "X_train_scaled_without_feed_in = X_train_scaled[:, [X_train.columns.get_loc(col) for col in cols_to_keep]]\n",
    "X_val_scaled_without_feed_in = X_val_scaled[:, [X_val.columns.get_loc(col) for col in cols_to_keep]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc8f36-117b-49bc-919e-279e9ab20343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize XGBoost Regressor model\n",
    "model = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    objective='reg:squarederror'\n",
    ")\n",
    "\n",
    "# train model on scaled train data\n",
    "model.fit(X_train_scaled_without_feed_in, y_Train)\n",
    "\n",
    "# predict y with validation data\n",
    "y_pred_wo_feed_in = model.predict(X_val_scaled_without_feed_in)\n",
    "\n",
    "# measure performance values\n",
    "mse = mean_squared_error(y_val, y_pred_wo_feed_in)\n",
    "r2 = r2_score(y_val, y_pred_wo_feed_in)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "print(f\"Validation MSE: {mse:.4f}\")\n",
    "print(f\"Validation R2: {r2:.4f}\")\n",
    "print(f\"Validation MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48533699-60f0-4453-bb5a-9f3b73c67b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: save model\n",
    "# model.save_model(\"01_Models/xgb_regressor_model_without_feed_in.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17fbbb-1967-4186-a139-74de535cb9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "feature_names = cols_to_keep\n",
    "\n",
    "# Sortieren\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "top_n = 15  # Anzahl der Top-Features\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(range(top_n), importances[sorted_idx][:top_n], color='skyblue')\n",
    "plt.xticks(range(top_n), [feature_names[i] for i in sorted_idx][:top_n], rotation=45, ha='right')\n",
    "plt.title('Top Feature Importances')\n",
    "plt.ylabel('Importance')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Werte über Balken anzeigen\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, height, f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b1099c-ca51-44ae-bf46-522608c6b61b",
   "metadata": {},
   "source": [
    "As you can see the model performs worse, because very important features are missing now. The key discipline is to improve this model. To visualize the model's performance, we will create scatter plots comparing the true values against the predicted values for both the model with and without the feed-in feature. This visualization will help us understand the model's accuracy and areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef42f17b-8fda-4269-ab72-1ee429bcd963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a scatterplot to visualize the true vs the predicted values of both models\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_val, y_pred_wo_feed_in, alpha=0.1, color='blue', label='Ohne Rückspeisemessug')\n",
    "plt.scatter(y_val, y_pred, alpha=0.1, color='green', label='Mit Rückspeisemessug')\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# without feed in feature\n",
    "axes[0].scatter(y_val, y_pred_wo_feed_in, alpha=0.1, color='blue')\n",
    "axes[0].plot([0, 12], [0, 12], linestyle='--', color='red', label='Perfektes Modell')\n",
    "axes[0].set_title('Ohne Rückspeisemessung')\n",
    "axes[0].set_xlabel('Wahre Werte')\n",
    "axes[0].set_ylabel('Vorhergesagte Werte')\n",
    "axes[0].legend()\n",
    "axes[0].grid()\n",
    "\n",
    "# model with feed in feature\n",
    "axes[1].scatter(y_val, y_pred, alpha=0.1, color='green')\n",
    "axes[1].plot([0, 12], [0, 12], linestyle='--', color='red', label='Perfektes Modell')\n",
    "axes[1].set_title('Mit Rückspeisemessung')\n",
    "axes[1].set_xlabel('Wahre Werte')\n",
    "axes[1].set_ylabel('Vorhergesagte Werte')\n",
    "axes[1].legend()\n",
    "axes[1].grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
